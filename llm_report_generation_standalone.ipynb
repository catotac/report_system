{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone LLM Report Generation Demo (Autoregressive, Multi-Iteration, Scores in Docx)\n",
    "\n",
    "This notebook demonstrates end-to-end autoregressive report generation, evaluation, self-reflection, and Word export using OpenAI and local prompt templates.\n",
    "\n",
    "- Each subsection is generated in 5 autoregressive self-reflection steps.\n",
    "- Each step uses the previous content and evaluation scores.\n",
    "- Separate prompt templates are used for generation and evaluation (per subsection if available).\n",
    "- The final docx includes all evaluation scores after each subsection.\n",
    "- All logic is contained in this notebook.\n",
    "- Prompt templates must be in `app/templates/`.\n",
    "- You need an OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import re\n",
    "from docx import Document\n",
    "from typing import List, Dict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set your OpenAI API key here\n",
    "openai.api_key = 'YOUR_OPENAI_KEY'  # <-- Replace with your key or use os.environ['OPENAI_API_KEY']\n",
    "TEMPLATE_DIR = 'app/templates'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Report Structure and Group"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sections = {\n",
    "    'Executive Summary': ['Overview', 'Key Points'],\n",
    "    'Business Goals': ['Objectives', 'KPIs'],\n",
    "    'Model Data': ['Data Sources', 'Data Quality'],\n",
    "    'Model Selection': ['Candidate Models', 'Selection Criteria'],\n",
    "    'Model Performance': ['Metrics', 'Validation Results'],\n",
    "    'Model Testing': ['Test Strategy', 'Test Results'],\n",
    "    'Model Monitoring': ['Monitoring Plan', 'Alerting & Retraining']\n",
    "}\n",
    "group_id = 'general'  # or 'credit', 'capital', etc.\n",
    "title = 'Standalone LLM Validation Report'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions: Prompt Template Loading, OpenAI Calls, Evaluation, Self-Reflection, Docx Export"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_group_template(doc_type: str, group_id: str, section: str = None, subsection: str = None, eval_mode=False) -> str:\n",
    "    # If eval_mode, look for _eval_prompt_template.txt\n",
    "    suffix = '_eval_prompt_template.txt' if eval_mode else '_prompt_template.txt'\n",
    "    candidates = []\n",
    "    if section and subsection:\n",
    "        candidates.append(f'{TEMPLATE_DIR}/{group_id}_{section}_{subsection}{suffix}')\n",
    "    if section:\n",
    "        candidates.append(f'{TEMPLATE_DIR}/{group_id}_{section}{suffix}')\n",
    "    candidates.append(f'{TEMPLATE_DIR}/{group_id}{suffix}')\n",
    "    candidates.append(f'{TEMPLATE_DIR}/{doc_type}{suffix}')\n",
    "    for path in candidates:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r') as f:\n",
    "                return f.read()\n",
    "    raise FileNotFoundError(f'No prompt template found for {group_id}, {section}, {subsection}, {doc_type}, eval_mode={eval_mode}')\n",
    "\n",
    "def call_openai(prompt: str, system_prompt: str = None) -> str:\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = openai.ChatCompletion.create(model='gpt-4', messages=messages)\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def parse_system_user_prompt(template: str):\n",
    "    sys, user = '', template\n",
    "    if '---SYSTEM PROMPT---' in template and '---USER PROMPT---' in template:\n",
    "        sys = template.split('---SYSTEM PROMPT---')[1].split('---USER PROMPT---')[0].strip()\n",
    "        user = template.split('---USER PROMPT---')[1].strip()\n",
    "    return sys, user\n",
    "\n",
    "def autoregressive_self_reflect(section, sub, context, group_id, doc_type='report', n_iter=5):\n",
    "    # Initial generation\n",
    "    template = load_group_template(doc_type, group_id, section, sub)\n",
    "    sys_prompt, user_prompt = parse_system_user_prompt(template)\n",
    "    prompt = user_prompt.replace('{{section}}', section).replace('{{subsection}}', sub).replace('{{context}}', context)\n",
    "    prompt = prompt.replace('{{previous_generation}}', '')\n",
    "    prompt = prompt.replace('{{groundedness_score}}', '0.0').replace('{{completeness_score}}', '0.0').replace('{{coherence_score}}', '0.0')\n",
    "    gen_text = call_openai(prompt, sys_prompt)\n",
    "    all_texts = [gen_text]\n",
    "    all_scores = []\n",
    "    for i in range(n_iter):\n",
    "        # Evaluation\n",
    "        eval_template = load_group_template(doc_type, group_id, section, sub, eval_mode=True)\n",
    "        eval_sys, eval_user = parse_system_user_prompt(eval_template)\n",
    "        eval_prompt = eval_user.replace('{{section}}', section).replace('{{subsection}}', sub).replace('{{context}}', context).replace('{{generated_text}}', all_texts[-1])\n",
    "        eval_response = call_openai(eval_prompt, eval_sys)\n",
    "        # Parse scores\n",
    "        def extract_first_float(line):\n",
    "            match = re.search(r'[-+]?[0-9]*\\.?[0-9]+', line)\n",
    "            return float(match.group()) if match else 0.0\n",
    "        lines = eval_response.split('\\n')\n",
    "        scores = []\n",
    "        for line in lines:\n",
    "            if len(scores) >= 3:\n",
    "                break\n",
    "            scores.append(extract_first_float(line))\n",
    "        while len(scores) < 3:\n",
    "            scores.append(0.0)\n",
    "        eval_scores = {\n",
    "            'groundedness': scores[0],\n",
    "            'completeness': scores[1],\n",
    "            'coherence': scores[2]\n",
    "        }\n",
    "        all_scores.append(eval_scores)\n",
    "        # Self-reflection\n",
    "        template = load_group_template(doc_type, group_id, section, sub)\n",
    "        sys_prompt, user_prompt = parse_system_user_prompt(template)\n",
    "        prompt = user_prompt.replace('{{section}}', section).replace('{{subsection}}', sub).replace('{{context}}', context)\n",
    "        prompt = prompt.replace('{{previous_generation}}', all_texts[-1])\n",
    "        prompt = prompt.replace('{{groundedness_score}}', str(eval_scores['groundedness']))\n",
    "        prompt = prompt.replace('{{completeness_score}}', str(eval_scores['completeness']))\n",
    "        prompt = prompt.replace('{{coherence_score}}', str(eval_scores['coherence']))\n",
    "        improved_text = call_openai(prompt, sys_prompt)\n",
    "        all_texts.append(improved_text)\n",
    "    return all_texts[-1], all_scores\n",
    "\n",
    "def export_to_docx(title, sections_output):\n",
    "    doc = Document()\n",
    "    doc.add_heading(title, 0)\n",
    "    current_section = None\n",
    "    for so in sections_output:\n",
    "        if so['section'] != current_section:\n",
    "            doc.add_heading(so['section'], level=1)\n",
    "            current_section = so['section']\n",
    "        doc.add_heading(so['subsection'], level=2)\n",
    "        doc.add_paragraph(so['generated_text'])\n",
    "        # Add evaluation scores\n",
    "        scores = so['scores'][-1] if so['scores'] else {'groundedness':0, 'completeness':0, 'coherence':0}\n",
    "        doc.add_paragraph(f\"Scores: Groundedness={scores['groundedness']}, Completeness={scores['completeness']}, Coherence={scores['coherence']}\", style='Intense Quote')\n",
    "    doc.save('standalone_generated_report.docx')\n",
    "    print('Saved as standalone_generated_report.docx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoregressive Generation, Evaluation, and Self-Reflection (5 Iterations)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sections_output = []\n",
    "context = ''\n",
    "for section, subs in sections.items():\n",
    "    for sub in subs:\n",
    "        final_text, all_scores = autoregressive_self_reflect(section, sub, context, group_id, doc_type='report', n_iter=5)\n",
    "        sections_output.append({\n",
    "            'section': section,\n",
    "            'subsection': sub,\n",
    "            'generated_text': final_text,\n",
    "            'scores': all_scores\n",
    "        })\n",
    "        context += final_text + '\n'\n",
    "        display(Markdown(f'### {section} / {sub}'))\n",
    "        display(Markdown(final_text))\n",
    "        if all_scores:\n",
    "            last = all_scores[-1]\n",
    "            display(Markdown(f\"*Groundedness: {last['groundedness']} | Completeness: {last['completeness']} | Coherence: {last['coherence']}*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export the Final Report as a Word Document (.docx) (with Scores)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_to_docx(title, sections_output)\n",
    "# Optionally, display the docx content\n",
    "from docx import Document as DocxDocument\n",
    "doc = DocxDocument('standalone_generated_report.docx')\n",
    "for para in doc.paragraphs:\n",
    "    print(para.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
 
